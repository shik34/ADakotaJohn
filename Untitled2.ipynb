{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPK+5AYS1Nt4wzsA65QjcN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shik34/ADakotaJohn/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embpUPt65WE1"
      },
      "source": [
        "from __future__ import print_function\n",
        "import shutil\n",
        "import os.path\n",
        "#import tensorflow as tf"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGjKaxiF7g5-"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqmAr8tj5x_V"
      },
      "source": [
        "import tensorflow_datasets\n",
        "mnist = tensorflow_datasets.load('mnist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90O4Fplo6dtG",
        "outputId": "1af026b1-e266-4b29-a990-84471a9560d7"
      },
      "source": [
        "mnist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " 'train': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "iAtMr_hP5T0u",
        "outputId": "3cf7876f-87a8-476e-a749-b7a682bafefa"
      },
      "source": [
        "from tensorflow.compat.v1.examples.tutorials.mnist import input_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-14d71b84d802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.compat.v1.examples'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiCq6Xuo6HwQ"
      },
      "source": [
        "EXPORT_DIR = './model'\n",
        "\n",
        "if os.path.exists(EXPORT_DIR):\n",
        "    shutil.rmtree(EXPORT_DIR)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "YPBM-8wM6MJv",
        "outputId": "3c7476fa-97c0-47e4-c8db-aa510418e09d"
      },
      "source": [
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a839aeb82f4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "xJ0txske5J74",
        "outputId": "a6a3a9df-0d80-4cdf-b92e-86858c723f07"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 200000\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "\n",
        "# Network Parameters\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.75  # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n",
        "\n",
        "\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create Model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "pred = conv_net(x, weights, biases, keep_prob)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    step = 1\n",
        "    # Keep training until reach max iterations\n",
        "    while step * batch_size < training_iters:\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
        "                                       keep_prob: dropout})\n",
        "        if step % display_step == 0:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y,\n",
        "                                                              keep_prob: 1.})\n",
        "            print(\"Iter \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.5f}\".format(acc))\n",
        "        step += 1\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 256 mnist test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "          sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
        "                                        y: mnist.test.labels[:256],\n",
        "                                        keep_prob: 1.}))\n",
        "    WC1 = weights['wc1'].eval(sess)\n",
        "    BC1 = biases['bc1'].eval(sess)\n",
        "    WC2 = weights['wc2'].eval(sess)\n",
        "    BC2 = biases['bc2'].eval(sess)\n",
        "    WD1 = weights['wd1'].eval(sess)\n",
        "    BD1 = biases['bd1'].eval(sess)\n",
        "    W_OUT = weights['out'].eval(sess)\n",
        "    B_OUT = biases['out'].eval(sess)\n",
        "\n",
        "# Create new graph for exporting\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "    x_2 = tf.placeholder(\"float\", shape=[None, 784], name=\"input\")\n",
        "\n",
        "    WC1 = tf.constant(WC1, name=\"WC1\")\n",
        "    BC1 = tf.constant(BC1, name=\"BC1\")\n",
        "    x_image = tf.reshape(x_2, [-1, 28, 28, 1])\n",
        "    CONV1 = conv2d(x_image, WC1, BC1)\n",
        "    MAXPOOL1 = maxpool2d(CONV1, k=2)\n",
        "\n",
        "    WC2 = tf.constant(WC2, name=\"WC2\")\n",
        "    BC2 = tf.constant(BC2, name=\"BC2\")\n",
        "    CONV2 = conv2d(MAXPOOL1, WC2, BC2)\n",
        "    MAXPOOL2 = maxpool2d(CONV2, k=2)\n",
        "\n",
        "    WD1 = tf.constant(WD1, name=\"WD1\")\n",
        "    BD1 = tf.constant(BD1, name=\"BD1\")\n",
        "\n",
        "    FC1 = tf.reshape(MAXPOOL2, [-1, WD1.get_shape().as_list()[0]])\n",
        "    FC1 = tf.add(tf.matmul(FC1, WD1), BD1)\n",
        "    FC1 = tf.nn.relu(FC1)\n",
        "\n",
        "    W_OUT = tf.constant(W_OUT, name=\"W_OUT\")\n",
        "    B_OUT = tf.constant(B_OUT, name=\"B_OUT\")\n",
        "\n",
        "    # skipped dropout for exported graph as there is no need for already calculated weights\n",
        "\n",
        "    OUTPUT = tf.nn.softmax(tf.matmul(FC1, W_OUT) + B_OUT, name=\"output\")\n",
        "\n",
        "    sess = tf.Session()\n",
        "    init = tf.initialize_all_variables()\n",
        "    sess.run(init)\n",
        "\n",
        "    graph_def = g.as_graph_def()\n",
        "    tf.train.write_graph(graph_def, EXPORT_DIR, 'mnist_model_graph.pb', as_text=False)\n",
        "\n",
        "    # Test trained model\n",
        "    y_train = tf.placeholder(\"float\", [None, 10])\n",
        "    correct_prediction = tf.equal(tf.argmax(OUTPUT, 1), tf.argmax(y_train, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "    print(\"check accuracy %g\" % accuracy.eval(\n",
        "            {x_2: mnist.test.images, y_train: mnist.test.labels}, sess))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-64a4bf0394f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Keep training until reach max iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtraining_iters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'train'"
          ]
        }
      ]
    }
  ]
}